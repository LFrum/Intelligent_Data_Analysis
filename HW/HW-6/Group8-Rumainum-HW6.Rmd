---
title: "Rumainum-HW6"
author: "Lince Rumainum"
date: "October 26, 2019"
output: word_document
---

## Intelligence Data Analysis - HW 6
  
```{r, echo=TRUE, fig.width=16, fig.height=10}
# list of libraries for HW5
#load all of the packages associated with the tidyverse
#includes dplyr, tidyr, and magritter
library(tidyverse)
#library(car) # for 'symbox' function
#library(EnvStats) # for "boxcox" function
library(ggplot2) # plots
library (VIM) # for aggr function
library (mice) # for imputation 
##pastecs has a function called stat.desc that provides one quick way at looking at some summary statistics
library(pastecs)
library (forcats) # for fct_lump function
library(sjmisc) # mmergeimputation

# for the non-linear modeling :
library(MASS) #for M-estimation method of robust regression: rlm
library(lars)
library(glmnet)
library(ISLR)
library(elasticnet)
library(earth) 
library(ggplot2)  #for pretty graphs
library(rgl)      #for rotating graphics library
# this is used for PCA plot but loaded later
# to avoid conflict with dplyr packege
#library(ggbiplot) # PCA plot
# source(s): https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html
################################################################################################################################################
# Problem 1 
################################################################################################################################################

########################################################################

# Train Data

########################################################################
# load training data
path <- "C:/Users/Lince/Documents/Fall 2019/DSA5103-IntelligentDataAnalysis/HW/HW-6/"
trainFile <- paste(path, "Train.csv", sep = "")
dfTrain <- read_csv(trainFile)

# make a copy of dfTrain
dfcTrain <- dfTrain
# get row length
data_length <- nrow(dfcTrain) 

# creating function for the mean of the missing values
myMeanNAsFun <- function(x) mean(is.na(x))

# see the percentage of missing data for each variable
apply(dfcTrain, 2, myMeanNAsFun)

# see column info
ncol(dfcTrain)
colnames(dfcTrain)

# create level for the channel column
channels <- fct_lump(dfTrain$channelGrouping, n = 7)
summary(channels) # see result summary

# create IDs for channel according to summary
dfcTrain$channelID <- NA
for (i in 1:data_length){
  if(is.na(channels[i])){
    dfcTrain$channelID[i] <- NA
  }
  else if(channels[i] == "Organic Search"){
    dfcTrain$channelID[i] <- 1
  }
  else if(channels[i] == "Social"){
    dfcTrain$channelID[i] <- 2
  }
  else if(channels[i] == "Referral"){
    dfcTrain$channelID[i] <- 3
  }
  else if(channels[i] == "Direct"){
    dfcTrain$channelID[i] <- 4
  }
  else if(channels[i] == "Paid Search"){
    dfcTrain$channelID[i] <- 5
  }
  else if(channels[i] == "Affiliates"){
    dfcTrain$channelID[i] <- 6
  }
  else if(channels[i] == "Display"){
    dfcTrain$channelID[i] <- 7
  }
  else if(channels[i] == "(Other)"){
    dfcTrain$channelID[i] <- 8
  }
}

# create IDs for the device column
devices <- fct_lump(dfTrain$deviceCategory, n = 3)
summary(devices) # see result summary

# create ID for deveice according to summary
dfcTrain$deviceID <- NA
for (i in 1:data_length){
  if(is.na(devices[i])){
    dfcTrain$deviceID[i] <- NA
  }
  else if(devices[i] == "desktop"){
    dfcTrain$deviceID[i] <- 1
  }
  else if(devices[i] == "mobile"){
    dfcTrain$deviceID[i] <- 2
  }
  else if(devices[i] == "tablet"){
    dfcTrain$deviceID[i] <- 3
  }
}

# create level for the source column
sources <- fct_lump(dfTrain$source, n = 9)
summary(sources) # see result summary

# create ID for the top 10 sources according to summary
dfcTrain$sourceID <- NA
for (i in 1:data_length){
  if(is.na(sources[i])){
    dfcTrain$sourceID[i] <- NA
  }
  else if(sources[i] == "google"){
    dfcTrain$sourceID[i] <- 1
  }
  else if(sources[i] == "youtube.com"){
    dfcTrain$sourceID[i] <- 2
  }
  else if(sources[i] == "(direct)"){
    dfcTrain$sourceID[i] <- 3
  }
  else if(sources[i] == "mall.googleplex.com"){
    dfcTrain$sourceID[i] <- 4
  }
  else if(sources[i] == "analytics.google.com"){
    dfcTrain$sourceID[i] <- 5
  }
  else if(sources[i] == "Partners"){
    dfcTrain$sourceID[i] <- 6
  }
  else if(sources[i] == "dfa"){
    dfcTrain$sourceID[i] <- 7
  }
  else if(sources[i] == "google.com"){
    dfcTrain$sourceID[i] <- 8
  }
  else if(sources[i] == "sites.google.com"){
    dfcTrain$sourceID[i] <- 9
  }
  else if(sources[i] == "Other"){
    dfcTrain$sourceID[i] <- 10
  }
}

# create level for the medium column
mediums <- fct_lump(dfTrain$medium, n = 5)
summary(mediums)# see result summary

# create ID for medium ID according to summary
dfcTrain$mediumID <- NA
for (i in 1:data_length){
  if(is.na(mediums[i])){
    dfcTrain$mediumID[i] <- NA
  }
  else if(mediums[i] == "organic"){
    dfcTrain$mediumID[i] <- 1
  }
  else if(mediums[i] == "referral"){
    dfcTrain$mediumID[i] <- 2
  }
  else if(mediums[i] == "cpc"){
    dfcTrain$mediumID[i] <- 3
  }
  else if(mediums[i] == "affiliate"){
    dfcTrain$mediumID[i] <- 4
  }
  else if(mediums[i] == "cpm"){
    dfcTrain$mediumID[i] <- 5
  }
}

# create level for country column
countries <- fct_lump(dfTrain$country, n = 9)
summary(countries) # see result summary

# create ID for the top 10 countries according to summary
dfcTrain$countryID <- NA
for (i in 1:data_length){
  if(is.na(countries[i])){
    dfcTrain$countryID[i] <- NA
  }
  else if(countries[i] == "United States"){
    dfcTrain$countryID[i] <- 1
  }
  else if(countries[i] == "India"){
    dfcTrain$countryID[i] <- 2
  }
  else if(countries[i] == "United Kingdom"){
    dfcTrain$countryID[i] <- 3
  }
  else if(countries[i] == "Canada"){
    dfcTrain$countryID[i] <- 4
  }
  else if(countries[i] == "Vietnam"){
    dfcTrain$countryID[i] <- 5
  }
  else if(countries[i] == "Thailand"){
    dfcTrain$countryID[i] <- 6
  }
  else if(countries[i] == "Turkey"){
    dfcTrain$countryID[i] <- 7
  }
  else if(countries[i] == "Germany"){
    dfcTrain$countryID[i] <- 8
  }
  else if(countries[i] == "Brazil"){
    dfcTrain$countryID[i] <- 9
  }
  else if(countries[i] == "Other"){
    dfcTrain$countryID[i] <- 10
  }
}

# create new feture usedAds based on all the column start with ad
# to see if data is used
# if data is there then 1, else otherwise
dfcTrain$usedAds <- NA
for (i in 1:data_length){
  if(!is.na(dfcTrain$adContent[i]) || !is.na(dfcTrain$adwordsClickInfo.adNetworkType[i]) ||
     !is.na(dfcTrain$adwordsClickInfo.gclId[i]) || !is.na(dfcTrain$adwordsClickInfo.isVideoAd[i]) ||
     !is.na(dfcTrain$adwordsClickInfo.page[i]) || !is.na(dfcTrain$adwordsClickInfo.slot[i])) {
    dfcTrain$usedAds[i] <- 1
  }
  else {
    dfcTrain$usedAds[i] <- 0
  }
}

# do imputations on all the NAs values
# modify them to 0s for bouces and newVisits columns
for (i in 1:data_length){
  if(is.na(dfcTrain$bounces[i])){
    dfcTrain$bounces[i] <- 0
  }
  
  if(is.na(dfcTrain$newVisits[i])){
    dfcTrain$newVisits[i] <- 0
  }
}

# check missing vlues on new features
apply(dfcTrain[,36:41],2,myMeanNAsFun)

dfcTrainAll <- dfcTrain # all the data with new features to a new df

# subset the training data
dfcTrain <- dfcTrainAll [ , c(1,2,3,6,7,14,21,32,33,34,36,37,38,39,40,41,35)]
# get all numeric columns only
dfTrainNum <- dfcTrain %>% select_if(is.numeric)
colnames(dfTrainNum) # see all the remaining column
# exclude sessionId
dfTrainNum <- dfTrainNum[,-1]

# create complete cases df (df without missing data)
dfcTrain_complete <- na.omit(dfTrainNum)
# total percentage of complete cases
nrow(dfcTrain_complete) / nrow(dfcTrain) * 100 

# create incomplete cases df
dfcTrain_incomplete <- dfTrainNum[!complete.cases(dfTrainNum),]
apply(dfcTrain_incomplete,2,myMeanNAsFun) # see the missingness within the df
# total percentage of incomplete cases
nrow(dfcTrain_incomplete) / nrow(dfcTrain) * 100

# use VIM package to see the missing data on the numeric data frame
md.pattern(dfTrainNum)
aggr_plot <- aggr(dfTrainNum, col=c('navyblue','red'), numbers = TRUE, sortVars = TRUE, 
                  labels=names(data), cex.axis=.6, gap = 3, ylab=c("Histogram of missing data","Pattern"))

# PCA
# compute pca using prcomp
dfcTrain_pca <- prcomp(dfcTrain_complete[,-1],scale = T)
summary(dfcTrain_pca) # summary of PCA

library(ggbiplot)
# biplot for PCA 1 and PCA 2
ggbiplot(dfcTrain_pca, obs.scale = 1, var.scale = 1, varname.size = 6, varname.adjust = 1.1, varname.abbrev = FALSE,
         labels.size = 10, alpha = 0.1, choices = (c(1,2)), circle = TRUE, ellipse = TRUE) +
  xlim(-3,5) +
  ylim(-5,5)
# to resolve error for dplyr
detach(package:ggbiplot)
detach(package:plyr)

# http://www.programmingr.com/how-to-calculate-mode-in-r/
getmode <- function(x) {
  keys <- na.omit(unique(x))
  keys[which.max(tabulate(match(x, keys)))]
}

# group all data by customerID
# taken the mean of the first three numeric columns
# taken the mode of the logical and factor columns
dfTr_custId <- dfTrainNum %>% 
  group_by(custId) %>%  
  summarize(visitNumber = mean(visitNumber), 
            timeSinceLastVisit = mean(timeSinceLastVisit),
            pageviews = mean(pageviews), 
            bounces = getmode(bounces), 
            newVisits = getmode(newVisits),
            channelID = getmode(channelID),
            deviceID = getmode(deviceID),
            sourceID = getmode(sourceID),
            mediumID = getmode(mediumID),
            countryID = getmode(countryID),
            usedAds = getmode(usedAds),
            revenue = sum(revenue))  %>% 
  arrange(custId)

# check missingness within the df
apply(dfTr_custId, 2, myMeanNAsFun)
# analyze each attribute behavior against the revenue and if ads was used 
dfTr_custId$usedAds = as.logical(dfTr_custId$usedAds) # change usedAds column to logical data type
# plot revenue vs visit number
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = visitNumber, y = revenue, color = usedAds)) +
  ylim(0,15000)
summary(dfTr_custId$visitNumber)
nrow(subset(dfTr_custId, visitNumber < 50))/nrow(dfTr_custId) * 100 # 99.98095 remains
nrow(subset(dfTr_custId, visitNumber < 25))/nrow(dfTr_custId) * 100 # 99.95132 remains
# plot revenue vs time Since Last Visit
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = timeSinceLastVisit, y = revenue, color = usedAds)) +
  ylim(0,6000)
nrow(subset(dfTr_custId, timeSinceLastVisit < 10000000))/nrow(dfTr_custId) * 100 # 99.97037 remains
# plot revenue vs page views
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = pageviews, y = revenue, color = usedAds)) +
  ylim(0,6000)
nrow(subset(dfTr_custId, pageviews < 150))/nrow(dfTr_custId) * 100 # 99.97884 remains
nrow(subset(dfTr_custId, pageviews < 100))/nrow(dfTr_custId) * 100 # 99.94921 remains
# plot revenue vs bounces
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = bounces, y = revenue, color = usedAds)) +
  ylim(0,6000)
# plot revenue vs new visits
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = newVisits, y = revenue, color = usedAds)) +
  ylim(0,6000)
# plot revenue vs each ID features
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = deviceID, y = revenue, color = usedAds)) +
  ylim(0,6000)
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = sourceID, y = revenue, color = usedAds)) +
  ylim(0,6000)
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = countryID, y = revenue, color = usedAds)) +
  ylim(0,6000)
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = channelID, y = revenue, color = usedAds)) +
  ylim(0,6000)
# channelID 5 - Paid Search is the only ID that used Ads
ggplot(data = dfTr_custId) + 
  geom_point(mapping = aes(x = mediumID, y = revenue, color = usedAds)) +
  ylim(0,6000)
# mediumID 3 - cpc is the only ID that used Ads

# summary of the numeric attributes
summary(dfTr_custId$visitNumber)
summary(dfTr_custId$pageviews)
summary(dfTr_custId$timeSinceLastVisit)

# create updated training data frame
#newTr_update1 <- subset(dfTr_cUStId, visitNumber < 50 & pageviews < 150 & revenue < 6000)
newTr_update1 <- subset(dfTr_custId, visitNumber < 25 & pageviews < 100 & revenue < 6000)
# log was taken on the first attributes of the df
newTr_update2 <- data.frame(log_visitnumber = log(newTr_update1$visitNumber+1), 
                            log_timesincelastvisit=log(newTr_update1$timeSinceLastVisit+1), 
                            log_pageviews=log(newTr_update1$pageviews+1),
                            channelID=newTr_update1$channelID, deviceID=newTr_update1$deviceID, 
                            sourceID=newTr_update1$sourceID, mediumID=newTr_update1$mediumID,
                            countryID=newTr_update1$countryID, 
                            bounces=newTr_update1$bounces,newVisits=newTr_update1$newVisits,
                            usedAds=newTr_update1$usedAds, revenue=newTr_update1$revenue)

# analyze data behavior after taking the log function of visitnumber, timesincelastvisit, pageviews
ggplot(data = newTr_update2) + 
  geom_point(mapping = aes(x = log_visitnumber, y = revenue, color = usedAds)) +
  ylim(0,6000)
ggplot(data = newTr_update2) + 
  geom_point(mapping = aes(x = log_timesincelastvisit, y = revenue, color = usedAds)) +
  ylim(0,6000)
ggplot(data = newTr_update2) + 
  geom_point(mapping = aes(x = log_pageviews, y = revenue, color = usedAds)) +
  ylim(0,6000)

# see missingness in new updated data
apply(newTr_update2,2,myMeanNAsFun)

################################################################################################################################################

# TEST DATA

################################################################################################################################################
# load test data
testFile <- paste(path, "Test.csv", sep = "")
dfTest <- read_csv(testFile)

# make a copy of dfTest
df_cTest <- dfTest
# get row length
test_length <- nrow(df_cTest)
# create an empty revenue column
df_cTest$revenue <- NA

# view number of column and its names
ncol(df_cTest)
colnames(df_cTest)

# use the same level of the channel column from train data
# create channel ID
df_cTest$channelID <- NA
for (i in 1:test_length){
  if(is.na(df_cTest$channelGrouping[i])){
    df_cTest$channelID[i] <- NA
  }
  else if(df_cTest$channelGrouping[i] == "Organic Search"){
    df_cTest$channelID[i] <- 1
  }
  else if(df_cTest$channelGrouping[i] == "Social"){
    df_cTest$channelID[i] <- 2
  }
  else if(df_cTest$channelGrouping[i] == "Referral"){
    df_cTest$channelID[i] <- 3
  }
  else if(df_cTest$channelGrouping[i] == "Direct"){
    df_cTest$channelID[i] <- 4
  }
  else if(df_cTest$channelGrouping[i] == "Paid Search"){
    df_cTest$channelID[i] <- 5
  }
  else if(df_cTest$channelGrouping[i] == "Affiliates"){
    df_cTest$channelID[i] <- 6
  }
  else if(df_cTest$channelGrouping[i] == "Display"){
    df_cTest$channelID[i] <- 7
  }
  else if(df_cTest$channelGrouping[i] == "(Other)"){
    df_cTest$channelID[i] <- 8
  }
}

# use the same level of the device column from train data
# create device ID 
df_cTest$deviceID <- NA
for (i in 1:test_length){
  if(is.na(df_cTest$deviceCategory[i])){
    df_cTest$deviceID[i] <- NA
  }
  else if(df_cTest$deviceCategory[i] == "desktop"){
    df_cTest$deviceID[i] <- 1
  }
  else if(df_cTest$deviceCategory[i] == "mobile"){
    df_cTest$deviceID[i] <- 2
  }
  else if(df_cTest$deviceCategory[i] == "tablet"){
    df_cTest$deviceID[i] <- 3
  }
}

# use the same level of the source column from train data
# create ID for the top 10 sources
df_cTest$sourceID <- NA

for (i in 1:test_length){
  if(is.na(df_cTest$source[i])){
    df_cTest$sourceID[i] <- NA
  }
  else if(df_cTest$source[i] == "google"){
    df_cTest$sourceID[i] <- 1
  }
  else if(df_cTest$source[i] == "youtube.com"){
    df_cTest$sourceID[i] <- 2
  }
  else if(df_cTest$source[i] == "(direct)"){
    df_cTest$sourceID[i] <- 3
  }
  else if(df_cTest$source[i] == "mall.googleplex.com"){
    df_cTest$sourceID[i] <- 4
  }
  else if(df_cTest$source[i] == "analytics.google.com"){
    df_cTest$sourceID[i] <- 5
  }
  else if(df_cTest$source[i] == "Partners"){
    df_cTest$sourceID[i] <- 6
  }
  else if(df_cTest$source[i] == "dfa"){
    df_cTest$sourceID[i] <- 7
  }
  else if(df_cTest$source[i] == "google.com"){
    df_cTest$sourceID[i] <- 8
  }
  else if(df_cTest$source[i] == "sites.google.com"){
    df_cTest$sourceID[i] <- 9
  }
  else if(df_cTest$source[i] == "Other"){
    df_cTest$sourceID[i] <- 10
  }
}

# create the same level of the medium column from the train data
df_cTest$mediumID <- NA
for (i in 1:test_length){
  if(is.na(df_cTest$medium[i])){
    df_cTest$mediumID[i] <- NA
  }
  else if(df_cTest$medium[i] == "organic"){
    df_cTest$mediumID[i] <- 1
  }
  else if(df_cTest$medium[i] == "referral"){
    df_cTest$mediumID[i] <- 2
  }
  else if(df_cTest$medium[i] == "cpc"){
    df_cTest$mediumID[i] <- 3
  }
  else if(df_cTest$medium[i] == "affiliate"){
    df_cTest$mediumID[i] <- 4
  }
  else if(df_cTest$medium[i] == "cpm"){
    df_cTest$mediumID[i] <- 5
  }
}

# create the same level of the country column from the train data
# create ID for the top 10 countries
df_cTest$countryID <- NA
for (i in 1:test_length){
  if(is.na(df_cTest$country[i])){
    df_cTest$countryID[i] <- NA
  }
  else if(df_cTest$country[i] == "United States"){
    df_cTest$countryID[i] <- 1
  }
  else if(df_cTest$country[i] == "India"){
    df_cTest$countryID[i] <- 2
  }
  else if(df_cTest$country[i] == "United Kingdom"){
    df_cTest$countryID[i] <- 3
  }
  else if(df_cTest$country[i] == "Canada"){
    df_cTest$countryID[i] <- 4
  }
  else if(df_cTest$country[i] == "Vietnam"){
    df_cTest$countryID[i] <- 5
  }
  else if(df_cTest$country[i] == "Thailand"){
    df_cTest$countryID[i] <- 6
  }
  else if(df_cTest$country[i] == "Turkey"){
    df_cTest$countryID[i] <- 7
  }
  else if(df_cTest$country[i] == "Germany"){
    df_cTest$countryID[i] <- 8
  }
  else if(df_cTest$country[i] == "Brazil"){
    df_cTest$countryID[i] <- 9
  }
  else if(df_cTest$country[i] == "Other"){
    df_cTest$countryID[i] <- 10
  }
}

# see if Ads was used in the data
df_cTest$usedAds <- NA
for (i in 1:test_length){
  # see if any ads was used
  if(!is.na(dfTest$adContent[i]) || !is.na(dfTest$adwordsClickInfo.adNetworkType[i]) ||
     !is.na(dfTest$adwordsClickInfo.gclId[i]) || !is.na(dfTest$adwordsClickInfo.isVideoAd[i]) ||
     !is.na(dfTest$adwordsClickInfo.page[i]) || !is.na(dfTest$adwordsClickInfo.slot[i])) {
    df_cTest$usedAds[i] <- 1
  }
  else {
    df_cTest$usedAds[i] <- 0
  }
}

# do imputations on all the NAs values
# modify them to 0s for bouces and newVisits columns
for (i in 1:test_length){
  if(is.na(df_cTest$bounces[i])){
    df_cTest$bounces[i] <- 0
  }
  
  if(is.na(df_cTest$newVisits[i])){
    df_cTest$newVisits[i] <- 0
  }
}

# check columns info
ncol(df_cTest)
colnames(df_cTest)

# check missing values for the new features
apply(df_cTest[,36:41],2,myMeanNAsFun)

dfcTestAll <- df_cTest # df for all test data with new features

# re-do start from here
# subset the test data
df_cTest <- dfcTestAll [ , c(1,2,3,6,7,14,21,32,33,34,36,37,38,39,40,41,35)]
df_cTest2 <- dfcTestAll [ , c(1,2,3,6,7,10,14,21,24,32,33,34,36,37,38,39,40,41,35)]
# get all numeric columns only
dfTestNum <- df_cTest %>% select_if(is.numeric)
dfTestNum2 <- df_cTest %>% select_if(is.numeric)
colnames(dfTestNum2)
# exclude sessionId
dfTestNum <- dfTestNum[,-1]

# create complete cases (df without missing data)
dfcTest_complete <- na.omit(dfTestNum)

# list rows of msleep data that have missing values
dfcTest_incomplete <- dfTestNum[!complete.cases(dfTestNum),]
apply(dfcTest_incomplete,2,myMeanNAsFun)


# use VIM package to see the missing data on the numeric data frame
md.pattern(dfTestNum)
aggr_plot <- aggr(dfTestNum, col=c('navyblue','red'), numbers = TRUE, sortVars = TRUE, 
                  labels=names(data), cex.axis=.6, gap = 3, ylab=c("Histogram of missing data","Pattern"))

# group all data by customerID
# taken the mean of the first three numeric columns
# taken the mode of the logical and factor columns
dfTest_custId <- dfTestNum %>% 
  group_by(custId) %>%  
  summarize(visitNumber = mean(visitNumber), 
            timeSinceLastVisit = mean(timeSinceLastVisit),
            pageviews = mean(pageviews), 
            bounces = getmode(bounces), 
            newVisits = getmode(newVisits),
            channelID = getmode(channelID),
            deviceID = getmode(deviceID),
            sourceID = getmode(sourceID),
            mediumID = getmode(mediumID),
            countryID = getmode(countryID),
            usedAds = getmode(usedAds)) %>%
  arrange(custId)


# create updated testing data frame
# on here, the log of visitNumber, timesincelastvisit, and pageViews are taken
# doing the same as how it was done on the training data
newTest_update2 <- data.frame(log_visitnumber = log(dfTest_custId$visitNumber+1), 
                              log_timesincelastvisit=log(dfTest_custId$timeSinceLastVisit+1), 
                              log_pageviews=log(dfTest_custId$pageviews+1),
                              channelID=dfTest_custId$channelID, deviceID=dfTest_custId$deviceID, 
                              sourceID=dfTest_custId$sourceID, mediumID=dfTest_custId$mediumID,
                              countryID=dfTest_custId$countryID, 
                              bounces=dfTest_custId$bounces,newVisits=dfTest_custId$newVisits,
                              usedAds=dfTest_custId$usedAds)

# create an empty revenue column
newTest_update2$revenue <- NA
nrow(newTest_update2)
# the linear model that was used is the training data with imputation
apply(newTest_update2,2,myMeanNAsFun) # check missingness

##############################################################################################################
########## Imputation using MICE of TRAIN data
# norm.predict is Linear regression, predicted values
# it creates 20 of multiple imputations and impute missing values for 5 iterations
#?mice
imp_tn <- mice(newTr_update2, m = 20, method = "sample", maxit = 5)
#imp_tn <- mice(newTr_update2, m = 5, method = "cart", maxit = 5)
#imp_tn <- mice(newTr_update2, m = 5, method = "rf", maxit = 10)

#plot those means and variances
#take a look at how the means and variances of the imputed values are (hopefully) converging 
plot(imp_tn) # it is converging

# take original data into imputed df
imputed_dfTrain <- as.data.frame(imp_tn$data)

# append imputed variables to imputed df
imputed_dfTrain <- merge_imputations(imputed_dfTrain, imp_tn, imputed_dfTrain)
# modify df accordingly
imputed_dfTrain<-imputed_dfTrain[,-c(6,7,8)]
colnames(imputed_dfTrain)[10] <- "sourceID"
colnames(imputed_dfTrain)[11] <- "mediumID"
colnames(imputed_dfTrain)[12] <- "countryID"
# see if there is still any NAs
apply(imputed_dfTrain,2,myMeanNAsFun)
apply(newTr_update2,2,myMeanNAsFun)

# view column names of newTr_update2
colnames(newTr_update2)

######################################################################################################
# used in HW-5 for the the linear model
# using data with imputations using mice norm.predict method
# create a linear model with the df's numeric attributes
lm2 = lm(log(revenue+1) ~ log_visitnumber + log_timesincelastvisit + log_pageviews +
           channelID + deviceID + sourceID + mediumID + countryID + 
           bounces + newVisits, data = imputed_dfTrain) 
# analysis the behavior of the linear model 2
summary(lm2)
par(mfrow=c(2,2)) 
plot(lm2)
par(mfrow=c(1,1)) 

######################################################################################################

# non-linear model

######################################################################################################
# matrices from imputed train data frame
imputed_dfTrain$usedAds = as.numeric(imputed_dfTrain$usedAds)
y_imp <- as.numeric (log(imputed_dfTrain[,9] + 1))
x_imp <- as.matrix(imputed_dfTrain[,c(1:8, 10, 11,12)])

# matrices from non-imputed train data frame
y_train <- as.numeric (log(newTr_update2[,9] + 1))
x_train <- as.matrix(newTr_update2[,c(1:8, 10, 11,12)])

# matrices for the test data
x_test <- as.matrix(newTest_update2[,c(1:11)]) 
y_test <- as.matrix(newTest_update2[,12]) 

######################################################################################################

# HUBER LOSS

######################################################################################################
# huber loss function
huberFit <- rlm(log(revenue+1) ~ log_visitnumber + log_timesincelastvisit + log_pageviews +
                  channelID + deviceID + sourceID + mediumID + countryID + 
                  bounces + newVisits,data = imputed_dfTrain, maxit = 100)

summary(huberFit)
hf_df <- newTest_update2
#apply model to the test data
revenue_huberFit = predict(huberFit, newdata = newTest_update2)

hf_df$revenue <- revenue_huberFit
apply(hf_df,2,myMeanNAsFun) 

#imputation on the test's data
imp_testH <- mice(hf_df, m = 30, method = "norm.predict", maxit = 5)
#imp_test2 <- mice(newTest_update2, m = 10, method = "cart", maxit = 5)

plot(imp_testH)

# create df for imputed data
imputed_dfTestAllH <- as.data.frame(imp_testH$data)

# append imputed variables to original data frame
imputed_dfTestAllH <- merge_imputations(imputed_dfTestAllH, imp_testH, imputed_dfTestAllH)
# modify df accordingly
#imputed_dfTest <- imputed_dfTestAll[,-c(3,6,7,8,12)]
imputed_dfTestH <- imputed_dfTestAllH[,-c(3,6,7,8)]
colnames(imputed_dfTestH)[9] <- "log_pageviews"
colnames(imputed_dfTestH)[10] <- "sourceID"
colnames(imputed_dfTestH)[11] <- "mediumID"
colnames(imputed_dfTestH)[12] <- "countryID"

revenue_huberFit_update = predict(huberFit, newdata = imputed_dfTestH)
results_huberFit<-tibble(custId=dfTest_custId$custId, predRevenue=revenue_huberFit_update)

#add a rule: if the predictions are negative, just make them 0
final_huberFit <- results_huberFit %>% mutate(predRevenue = replace(predRevenue, which(predRevenue<0), 0))

nrow(final_huberFit)
apply(final_huberFit,2,myMeanNAsFun) 

# write submission to csv file
path <- "C:\\Users\\Lince\\Documents\\Fall 2019\\DSA5103-IntelligentDataAnalysis\\HW\\HW-6\\submissions\\"
submissionFile <- paste(path, "Submission-huberFit.csv", sep = "")

submission <- final_huberFit
write.csv(submission, submissionFile, row.names = F)

########################################################################################################################

# MARS

########################################################################################################################
# mars fit
marsFit <- earth(log(revenue+1) ~ log_visitnumber + log_timesincelastvisit + log_pageviews +
                   channelID + deviceID + sourceID + mediumID + countryID + 
                   bounces + newVisits, imputed_dfTrain)
summary(marsFit)  

# predict revenue
results_marsFit<-tibble(custId=dfTest_custId$custId, predRevenue=predict(marsFit,newTest_update2))
apply(results_marsFit,2,myMeanNAsFun) 

test_marsFit <- newTest_update2
test_marsFit$revenue <- as.numeric(as.list(results_marsFit$predRevenue))

# imputated the missing data
imp_testM <- mice(test_marsFit, m = 50, method = "norm.predict", maxit = 10)
#imp_testM <- mice(test_marsFit, m = 10, method = "cart", maxit = 5)
plot(imp_testM) # see if it is converging

# create df for imputed data
imputed_dfTestAll_mars <- as.data.frame(imp_testM$data)

# append imputed variables to original data frame
imputed_dfTestAll_mars <- merge_imputations(imputed_dfTestAll_mars, imp_testM, imputed_dfTestAll_mars)
nrow(imputed_dfTestAll_mars)
# modify df accordingly
imputed_dfTest_mars <- imputed_dfTestAll_mars[,-c(3,6,7,8,12)]
colnames(imputed_dfTest_mars)[8] <- "log_pageviews"
colnames(imputed_dfTest_mars)[9] <- "sourceID"
colnames(imputed_dfTest_mars)[10] <- "mediumID"
colnames(imputed_dfTest_mars)[11] <- "countryID"
colnames(imputed_dfTest_mars)[12] <- "revenue"
nrow(imputed_dfTest_mars)

# update results
results_marsFit<-tibble(custId=dfTest_custId$custId, predRevenue=imputed_dfTest_mars$revenue)
# all revenue < 0 change to 0
final_marsFit <- results_marsFit %>% mutate(predRevenue = replace(predRevenue, which(predRevenue<0), 0))

# write submission to csv file
path <- "C:\\Users\\Lince\\Documents\\Fall 2019\\DSA5103-IntelligentDataAnalysis\\HW\\HW-6\\submissions\\"
submissionFile <- paste(path, "Submission-marsFit.csv", sep = "")

submission <- final_marsFit
write.csv(submission, submissionFile, row.names = F)

###############################################################################################

# RIDGE MODEL

###############################################################################################
# create grid for lambdas
grid = 10^seq(10, -10, length = 100)

## Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
ridge_mod = glmnet(x_imp, y_imp, alpha = 0, lambda = grid, thresh = 1e-12)
#find the best lambda from our list via cross-validation
cv.out <- cv.glmnet(x_imp, y_imp, alpha = 0, lambda = grid, thresh = 1e-12)
bestlam <- cv.out$lambda.min
plot(cv.out)

ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test)
ridge_df <- newTest_update2


#apply model to the test data
results_ridge<-tibble(custId=dfTest_custId$custId, predRevenue = ridge_pred)
apply(results_ridge,2,myMeanNAsFun) 

test_ridge <- newTest_update2
test_ridge$revenue <- as.numeric(as.list(results_ridge$predRevenue))

# imputated the missing data
imp_testR <- mice(test_ridge, m = 25, method = "sample", maxit = 10)
#imp_testM <- mice(test_marsFit, m = 10, method = "cart", maxit = 5)
plot(imp_testR) # see if it is converging

# create df for imputed data
imputed_dfTestAll_ridge <- as.data.frame(imp_testR$data)

# append imputed variables to original data frame
imputed_dfTestAll_ridge <- merge_imputations(imputed_dfTestAll_ridge, imp_testR, imputed_dfTestAll_ridge)
nrow(imputed_dfTestAll_ridge)
# modify df accordingly
imputed_dfTest_ridge <- imputed_dfTestAll_ridge[,-c(3,6,7,8,12)]
colnames(imputed_dfTest_ridge)[8] <- "log_pageviews"
colnames(imputed_dfTest_ridge)[9] <- "sourceID"
colnames(imputed_dfTest_ridge)[10] <- "mediumID"
colnames(imputed_dfTest_ridge)[11] <- "countryID"
colnames(imputed_dfTest_ridge)[12] <- "revenue"
nrow(imputed_dfTest_ridge)

# update results
results_ridge<-tibble(custId=dfTest_custId$custId, predRevenue=imputed_dfTest_ridge$revenue)
# all revenue < 0 change to 0
final_ridge <- results_ridge %>% mutate(predRevenue = replace(predRevenue, which(predRevenue<0), 0))
apply(final_ridge,2,myMeanNAsFun) 

# write submission to csv file
path <- "C:\\Users\\Lince\\Documents\\Fall 2019\\DSA5103-IntelligentDataAnalysis\\HW\\HW-6\\submissions\\"
submissionFile <- paste(path, "Submission-ridgeFit.csv", sep = "")

submission <- final_ridge
write.csv(submission, submissionFile, row.names = F)

######################################################################################################

# LASSO MODEL 1 USING CV LARS

######################################################################################################
cvlas<-cv.lars(x_imp,y_imp,type="lasso",mode="fraction")
cvlas
opt.frac <- min(cvlas$cv) + sd(cvlas$cv) 
opt.frac <- cvlas$index[which(cvlas$cv < opt.frac)[1]]
lasso.path <- lars(x_imp, y_imp, type = "lasso")
summary(lasso.path)
lasso.fit <- predict.lars(lasso.path, type = "coefficients", mode = "fraction", s = opt.frac)
coef(lasso.fit)
lasso.fit

# predict revenue using lasso
revenue_lassoFit_update = predict.lars(lasso.path,  type = "fit", newx = x_test)
test_lassoFit = newTest_update2
test_lassoFit$revenue = as.numeric(as.list(revenue_lassoFit_update$fit[,12]))
apply(test_lassoFit,2,myMeanNAsFun) 
nrow(test_lassoFit)

#imputation on the test's data
#imp_testL <- mice(test_lassoFit, m = 20, method = "norm.predict", maxit = 15)
imp_testL <- mice(test_lassoFit, m = 50, method = "sample", maxit = 5)
plot(imp_testL)

# create df for imputed data
imputed_dfTestAll_lasso <- as.data.frame(imp_testL$data)

# append imputed variables to original data frame
imputed_dfTestAll_lasso <- merge_imputations(imputed_dfTestAll_lasso, imp_testL, imputed_dfTestAll_lasso)
nrow(imputed_dfTestAll_lasso)
# modify df accordingly
#imputed_dfTest <- imputed_dfTestAll[,-c(3,6,7,8,12)]
imputed_dfTest_lasso <- imputed_dfTestAll_lasso[,-c(3,6,7,8,12)]
colnames(imputed_dfTest_lasso)[8] <- "log_pageviews"
colnames(imputed_dfTest_lasso)[9] <- "sourceID"
colnames(imputed_dfTest_lasso)[10] <- "mediumID"
colnames(imputed_dfTest_lasso)[11] <- "countryID"
colnames(imputed_dfTest_lasso)[12] <- "revenue"
nrow(imputed_dfTest_lasso)

# update revenue
revenue_lassoFit_update<-tibble(custId=dfTest_custId$custId, predRevenue=imputed_dfTest_lasso$revenue)

# predRevenue < 0, change to 0
final_lassoFit <- revenue_lassoFit_update %>% mutate(predRevenue = replace(predRevenue, which(predRevenue<0), 0))

nrow(final_lassoFit)
apply(final_lassoFit,2,myMeanNAsFun) 

# write submission to csv file
path <- "C:\\Users\\Lince\\Documents\\Fall 2019\\DSA5103-IntelligentDataAnalysis\\HW\\HW-6\\submissions\\"
submissionFile <- paste(path, "Submission-lasso-1.csv", sep = "")

submission <- final_lassoFit
write.csv(submission, submissionFile, row.names = F)

###############################################################################################

# LASSO MODEL - 2 USING GLMNET

###############################################################################################
lasso_mod2 = glmnet(x_imp, y_imp, alpha = 1, lambda = grid) # Fit lasso model on training data

plot(lasso_mod2)    # Draw plot of coefficients
set.seed(100)
cv.out = cv.glmnet(x_imp, y_imp, alpha = 1) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE
lasso_pred2 = predict(lasso_mod2, s = bestlam, newx = x_test) # Use best lambda to predict test data

lasso_coef2 = predict(cv.out, type = "coefficients", s = bestlam)[1:11,] # Display coefficients using lambda chosen by CV
lasso_coef2

lasso_coef2[lasso_coef2 != 0] # Display only non-zero coefficients

# put results in df
revenue_lassoFit_update2 <- tibble(custId=dfTest_custId$custId, predRevenue=lasso_pred2)
apply(revenue_lassoFit_update2,2,myMeanNAsFun) 
test_lassoFit2 = newTest_update2
test_lassoFit2$revenue = as.numeric(as.list(lasso_pred2))
apply(test_lassoFit2,2,myMeanNAsFun) 

#imputation on the test's data
#imp_testL <- mice(test_lassoFit, m = 20, method = "norm.predict", maxit = 15)
imp_testL2 <- mice(test_lassoFit2, m = 50, method = "sample", maxit = 5)
plot(imp_testL2)

# create df for imputed data
imputed_dfTestAll_lasso2 <- as.data.frame(imp_testL2$data)

# append imputed variables to original data frame
imputed_dfTestAll_lasso2 <- merge_imputations(imputed_dfTestAll_lasso2, imp_testL2, imputed_dfTestAll_lasso2)
nrow(imputed_dfTestAll_lasso2)
# modify df accordingly

imputed_dfTest_lasso2 <- imputed_dfTestAll_lasso2[,-c(3,6,7,8,12)]
colnames(imputed_dfTest_lasso2)[8] <- "log_pageviews"
colnames(imputed_dfTest_lasso2)[9] <- "sourceID"
colnames(imputed_dfTest_lasso2)[10] <- "mediumID"
colnames(imputed_dfTest_lasso2)[11] <- "countryID"
colnames(imputed_dfTest_lasso2)[12] <- "revenue"
nrow(imputed_dfTest_lasso2)

# update revenue
revenue_lassoFit_update2 <-tibble(custId=dfTest_custId$custId, predRevenue=imputed_dfTest_lasso2$revenue)

# predRevenue < 0, change to 0
final_lassoFit2 <- revenue_lassoFit_update2 %>% mutate(predRevenue = replace(predRevenue, which(predRevenue<0), 0))

nrow(final_lassoFit2)
apply(final_lassoFit2,2,myMeanNAsFun) 

# write submission to csv file
path <- "C:\\Users\\Lince\\Documents\\Fall 2019\\DSA5103-IntelligentDataAnalysis\\HW\\HW-6\\submissions\\"
submissionFile <- paste(path, "Submission-lasso-2.csv", sep = "")

submission <- final_lassoFit2
write.csv(submission, submissionFile, row.names = F)

##############################################################################################################

# graphs of ridge and lasso for different lamdas

##############################################################################################################
# 10-fold CV for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.ridge <- glmnet(x_imp, y_imp, family = "gaussian", alpha = 0)
fit.ridge.cv <- cv.glmnet(x_imp, y_imp, type.measure="mse", alpha = 0,
                          family = "gaussian")

fit.lasso <- glmnet(x_imp, y_imp, family = "gaussian", alpha = 1)
fit.lasso.cv <- cv.glmnet(x_imp, y_imp, type.measure = "mse", alpha = 1, 
                          family = "gaussian")

for (i in 0:10) {
  assign(paste("fit", i, sep=""), 
         cv.glmnet(x_imp, y_imp, type.measure = "mse", 
                   alpha = i/10, family = "gaussian"))
}

# Plot solution paths:
par(mfrow=c(3,2))
# ridge
plot(fit.ridge, xvar="lambda")
plot(fit0, main="Ridge")
# lasso
plot(fit.lasso, xvar="lambda")
plot(fit10, main="LASSO")

####################
# END OF PROBLEM 1 #
####################
```